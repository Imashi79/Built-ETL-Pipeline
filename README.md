# 🚀 ETL Pipelines for Data Processing  

This repository contains multiple **independent ETL (Extract, Transform, Load) pipelines**, designed to process data from various sources, including APIs, CSV, and Excel files.  

## 📁 Project Structure  

Each folder represents an **independent ETL pipeline** with its own implementation:  

- 📌 **ETL_pipeline_api/** – Extracts data from an API, transforms it, and loads it into a database.  
- 📌 **ETL_pipeline_csv_1/** – Processes raw CSV files, cleans, and stores data for analysis.  
- 📌 **ETL_pipeline_csv_2/** – A variation of the CSV-based ETL pipeline with different transformation logic.  
- 📌 **ETL_pipeline_excel/** – Reads Excel files, performs data wrangling, and loads structured data.  

## 🔧 Technologies Used  

- ✅ **Python** *(pandas, requests, sqlalchemy, psycopg2)*  
- ✅ **PostgreSQL** *(for structured data storage)*  
- ✅ **Jupyter Notebook** *(for development and testing)*  

## 🔍 Overview of ETL Process  

1️⃣ **Extract** – Fetches data from APIs, CSV, or Excel sources.  
2️⃣ **Transform** – Cleans, filters, and structures the data.  
3️⃣ **Load** – Stores processed data in a PostgreSQL database for further analysis.  

## 🎯 Expected Outcome  

- ✅ Well-structured, cleaned, and formatted data.  
- ✅ Scalable and modular ETL pipelines.  
- ✅ Ready-to-use datasets stored in a relational database for querying and analytics.  

---


